# PerceptrÃ³n Multicapa (MLP) - TP 2025

ImplementaciÃ³n de un PerceptrÃ³n Multicapa para clasificaciÃ³n de patrones de letras (B, D, F).

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Script Interactivo (Recomendado)

```bash
python entrenar_mlp.py
```

Este script te guiarÃ¡ paso a paso:
1. âœ… **Selecciona cantidad**: 100 / 500 / 1000 ejemplos
2. âœ… **Configura distorsiÃ³n**: 1-30%
3. âœ… **Define arquitectura**: 1 o 2 capas ocultas
4. âœ… **Ajusta hiperparÃ¡metros**: learning rate, momentum, epochs
5. âœ… **Entrena y evalÃºa** automÃ¡ticamente

### OpciÃ³n 2: Jupyter Notebook

```bash
jupyter notebook flujo_completo.ipynb
```

El notebook incluye:
- GeneraciÃ³n de datos
- Visualizaciones
- Entrenamiento paso a paso
- AnÃ¡lisis de resultados

## ğŸ“¦ InstalaciÃ³n

1. Crear un entorno virtual:
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac
```

2. Instalar dependencias:
```bash
pip install -r requirements.txt
```

## ğŸ“Š SelecciÃ³n de Cantidad de Ejemplos

El proyecto soporta **3 tamaÃ±os de dataset**:

| Cantidad | Tiempo de entrenamiento | Uso recomendado |
|----------|------------------------|------------------|
| **100** | RÃ¡pido (segundos) | Pruebas y desarrollo |
| **500** | Medio (1-2 min) | **Recomendado** - Equilibrado |
| **1000** | Completo (5+ min) | MÃ¡xima precisiÃ³n |

### En el script interactivo:

```python
python entrenar_mlp.py
# Te preguntarÃ¡: Â¿QuÃ© cantidad deseas usar? (1/2/3) [default=2]
```

### En el notebook:

```python
# Celda de configuraciÃ³n (ajustar MODO_INTERACTIVO)
MODO_INTERACTIVO = True  # Input manual
# o
MODO_INTERACTIVO = False  # Usar cantidad predefinida
cantidad = 500  # Cambiar aquÃ­: 100, 500 o 1000
```

## ğŸ—ï¸ Estructura del Proyecto

```
perceptron2/
â”œâ”€â”€ mlp.py                      # Clase MLP principal
â”œâ”€â”€ generador_dataset.py        # GeneraciÃ³n de datasets
â”œâ”€â”€ distorsionador.py          # DistorsiÃ³n inteligente (1sâ†’0s)
â”œâ”€â”€ clasificador.py            # Clasificador de letras
â”œâ”€â”€ entrenar_mlp.py            # ğŸ†• Script interactivo de entrenamiento
â”œâ”€â”€ comparar_distorsiones.py   # ComparaciÃ³n visual de mÃ©todos
â”œâ”€â”€ flujo_completo.ipynb       # Notebook completo
â”œâ”€â”€ requirements.txt           # Dependencias
â””â”€â”€ data/
    â”œâ”€â”€ originales/
    â”‚   â”œâ”€â”€ 100/letras.csv
    â”‚   â”œâ”€â”€ 500/letras.csv
    â”‚   â””â”€â”€ 1000/letras.csv
    â””â”€â”€ distorsionadas/
        â”œâ”€â”€ 100/letras.csv
        â”œâ”€â”€ 500/letras.csv
        â””â”€â”€ 1000/letras.csv
```

## ğŸ¯ CaracterÃ­sticas Implementadas

### âœ… 1. Arquitecturas Flexibles

```python
from mlp import MLP

# 1 capa oculta (simple y rÃ¡pida)
mlp = MLP(capas_ocultas=[8])  # 100 â†’ 8 â†’ 3

# 2 capas ocultas (mÃ¡s capacidad)
mlp = MLP(capas_ocultas=[10, 8])  # 100 â†’ 10 â†’ 8 â†’ 3
```

**API Simplificada**: Solo especificas capas ocultas, entrada (100) y salida (3) son fijos.

### âœ… 2. Dos MÃ©todos de DistorsiÃ³n

#### MÃ©todo ClÃ¡sico:
```python
generador.generar_data_distorsionadas(cant=500, min_distorsion=0.01, max_distorsion=0.30)
```
- InversiÃ³n aleatoria (0â†”1)

#### MÃ©todo Distorsionador (Recomendado):
```python
generador.generar_data_distorsionadas_v2(cant=500, min_distorsion=5.0, max_distorsion=25.0)
```
- Intercambio inteligente (1sâ†’0s)
- MÃ¡s realista para degradaciÃ³n visual

### âœ… 3. Backpropagation Optimizado

- Operaciones vectorizadas (100-1000x mÃ¡s rÃ¡pido)
- Separado en `backward_propagation()` y `gradiente_descendente()`
- Momentum estÃ¡ndar implementado correctamente

### âœ… 4. Monitoreo de Entrenamiento

```python
historial = mlp.entrenar(X, y, epochs=50, verbose=True)
# Muestra progreso en CADA Ã©poca (no cada 100)
```

### âœ… 5. SelecciÃ³n Interactiva de Cantidad

**Tres opciones disponibles**:
- ğŸ”¹ **100 ejemplos**: Pruebas rÃ¡pidas
- ğŸ”¹ **500 ejemplos**: Equilibrado (recomendado)
- ğŸ”¹ **1000 ejemplos**: Dataset completo

## ğŸ“‹ Requisitos del Proyecto (Cumplidos)

### Datasets
- 3 datasets con 100, 500 y 1000 ejemplos
- Patrones en matriz 10x10 (letras b, d, f)
- 10% patrones sin distorsiÃ³n
- 90% con distorsiÃ³n del 1% al 30%

### Entrenamiento
- 3 conjuntos de validaciÃ³n por dataset (10%, 20%, 30%)
- 1 o 2 capas ocultas
- 5 a 10 neuronas por capa
- Funciones de activaciÃ³n: lineal y sigmoidal
- Learning rate: 0 a 1
- Momentum: 0 a 1

### Reconocimiento
- PatrÃ³n distorsionado del 0% al 30% (generado automÃ¡tica o manualmente)

## PrÃ³ximos Pasos

1. âœ… Implementar clase MLP con arquitectura configurable
2. â³ Generar datasets de patrones (b, d, f)
3. â³ Implementar interfaz de usuario
4. â³ Sistema de evaluaciÃ³n y mÃ©tricas (MSE, error de entrenamiento, validaciÃ³n)
5. â³ GeneraciÃ³n de informes

## Autor

Trabajo PrÃ¡ctico - Inteligencia Artificial 2025
