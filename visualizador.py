"""
Utilidades para visualizaci√≥n de resultados del entrenamiento del MLP.

Responsabilidades:
- Visualizar curvas de aprendizaje (MSE)
- Visualizar distribuciones de datos
- Visualizar patrones de letras
- Visualizar m√©tricas de rendimiento
- Crear matrices de confusi√≥n
"""

import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Union


def graficar_mse_entrenamiento_validacion(historial: Dict[str, List[float]], 
                                          titulo: str = "MSE de Entrenamiento vs Validaci√≥n",
                                          guardar_como: str = None):
    """
    Grafica las curvas de MSE para entrenamiento y validaci√≥n.
    
    Par√°metros:
    -----------
    historial : dict
        Diccionario con keys 'train_loss' y 'val_loss'
    titulo : str
        T√≠tulo del gr√°fico
    guardar_como : str, opcional
        Ruta del archivo para guardar la imagen
    """
    plt.figure(figsize=(10, 6))
    
    epochs = range(1, len(historial['train_loss']) + 1)
    
    plt.plot(epochs, historial['train_loss'], 'g-', label='Entrenamiento', linewidth=2)
    plt.plot(epochs, historial['val_loss'], 'b-', label='Validaci√≥n', linewidth=2)
    
    plt.xlabel('√âpocas', fontsize=12)
    plt.ylabel('Error global', fontsize=12)
    plt.title(titulo, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right', fontsize=11)
    plt.grid(True, alpha=0.3)
    
    # Agregar anotaciones de valores inicial y final
    plt.annotate(f'{historial["train_loss"][0]:.4f}', 
                xy=(1, historial['train_loss'][0]), 
                xytext=(5, 10), textcoords='offset points', fontsize=9)
    plt.annotate(f'{historial["train_loss"][-1]:.4f}', 
                xy=(len(epochs), historial['train_loss'][-1]), 
                xytext=(5, 10), textcoords='offset points', fontsize=9)
    
    plt.annotate(f'{historial["val_loss"][0]:.4f}', 
                xy=(1, historial['val_loss'][0]), 
                xytext=(5, -15), textcoords='offset points', fontsize=9)
    plt.annotate(f'{historial["val_loss"][-1]:.4f}', 
                xy=(len(epochs), historial['val_loss'][-1]), 
                xytext=(5, -15), textcoords='offset points', fontsize=9)
    
    plt.tight_layout()
    
    if guardar_como:
        plt.savefig(guardar_como, dpi=300, bbox_inches='tight')
        print(f"üìä Gr√°fica guardada en: {guardar_como}")
    
    plt.show()


def graficar_comparacion_splits(resultados: Dict[str, Dict], 
                                titulo: str = "Comparaci√≥n de Splits de Validaci√≥n"):
    """
    Grafica la comparaci√≥n de diferentes splits de validaci√≥n.
    
    Par√°metros:
    -----------
    resultados : dict
        Diccionario con formato:
        {
            '10%': {'train_loss': [...], 'val_loss': [...]},
            '20%': {'train_loss': [...], 'val_loss': [...]},
            '30%': {'train_loss': [...], 'val_loss': [...]}
        }
    titulo : str
        T√≠tulo del gr√°fico
    """
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(titulo, fontsize=16, fontweight='bold')
    
    colores = {'train': 'g', 'val': 'b'}
    
    for idx, (split_name, historial) in enumerate(resultados.items()):
        ax = axes[idx]
        epochs = range(1, len(historial['train_loss']) + 1)
        
        ax.plot(epochs, historial['train_loss'], f'{colores["train"]}-', 
                label='Entrenamiento', linewidth=2)
        ax.plot(epochs, historial['val_loss'], f'{colores["val"]}-', 
                label='Validaci√≥n', linewidth=2)
        
        ax.set_xlabel('√âpocas', fontsize=11)
        ax.set_ylabel('Error global', fontsize=11)
        ax.set_title(f'Split: {split_name} validaci√≥n', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right', fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # Calcular gap final
        gap = historial['val_loss'][-1] - historial['train_loss'][-1]
        ax.text(0.5, 0.95, f'Gap final: {gap:.4f}', 
                transform=ax.transAxes, fontsize=10,
                verticalalignment='top', horizontalalignment='center',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.show()


def graficar_mse_simple(historial: Union[List[float], Dict[str, List[float]]], 
                       titulo: str = "Evoluci√≥n del Error (MSE)",
                       guardar_como: str = None):
    """
    Grafica la evoluci√≥n del MSE (solo entrenamiento o con validaci√≥n).
    
    Par√°metros:
    -----------
    historial : list o dict
        Lista de errores [e1, e2, ...] o dict {'train_loss': [...], 'val_loss': [...]}
    titulo : str
        T√≠tulo del gr√°fico
    guardar_como : str, opcional
        Ruta del archivo para guardar la imagen
    """
    plt.figure(figsize=(10, 6))
    
    if isinstance(historial, dict):
        # Historial con validaci√≥n
        epochs = range(1, len(historial['train_loss']) + 1)
        plt.plot(epochs, historial['train_loss'], 'g-', label='Entrenamiento', linewidth=2)
        plt.plot(epochs, historial['val_loss'], 'b-', label='Validaci√≥n', linewidth=2)
        plt.legend(loc='upper right', fontsize=11)
    else:
        # Historial solo de entrenamiento
        epochs = range(1, len(historial) + 1)
        plt.plot(epochs, historial, 'g-', label='Error', linewidth=2)
    
    plt.xlabel('√âpocas', fontsize=12)
    plt.ylabel('Error (MSE)', fontsize=12)
    plt.title(titulo, fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if guardar_como:
        plt.savefig(guardar_como, dpi=300, bbox_inches='tight')
        print(f"üìä Gr√°fica guardada en: {guardar_como}")
    
    plt.show()


def analizar_overfitting(historial: Dict[str, List[float]], 
                        umbral_gap: float = 0.05) -> Dict:
    """
    Analiza si hay overfitting comparando train y validation loss.
    
    Par√°metros:
    -----------
    historial : dict
        Diccionario con 'train_loss' y 'val_loss'
    umbral_gap : float
        Umbral para considerar overfitting
    
    Retorna:
    --------
    dict
        Diccionario con an√°lisis del overfitting
    """
    train_final = historial['train_loss'][-1]
    val_final = historial['val_loss'][-1]
    gap = val_final - train_final
    
    # Calcular mejora
    train_inicial = historial['train_loss'][0]
    val_inicial = historial['val_loss'][0]
    mejora_train = ((train_inicial - train_final) / train_inicial) * 100
    mejora_val = ((val_inicial - val_final) / val_inicial) * 100
    
    # Determinar estado
    if gap > umbral_gap:
        estado = "‚ö†Ô∏è  OVERFITTING DETECTADO"
        descripcion = "El modelo se ajusta demasiado a los datos de entrenamiento"
    elif val_final < train_final:
        estado = "‚úÖ EXCELENTE"
        descripcion = "El modelo generaliza mejor de lo esperado"
    else:
        estado = "‚úÖ BUEN BALANCE"
        descripcion = "Balance saludable entre entrenamiento y validaci√≥n"
    
    return {
        'estado': estado,
        'descripcion': descripcion,
        'gap': gap,
        'train_final': train_final,
        'val_final': val_final,
        'mejora_train': mejora_train,
        'mejora_val': mejora_val
    }


def imprimir_resumen_entrenamiento(historial: Dict[str, List[float]]):
    """
    Imprime un resumen detallado del entrenamiento.
    
    Par√°metros:
    -----------
    historial : dict
        Diccionario con 'train_loss' y 'val_loss'
    """
    analisis = analizar_overfitting(historial)
    
    print("\n" + "="*70)
    print("üìä RESUMEN DEL ENTRENAMIENTO")
    print("="*70)
    
    print(f"\nüìâ Error Entrenamiento:")
    print(f"   - Inicial: {historial['train_loss'][0]:.6f}")
    print(f"   - Final:   {historial['train_loss'][-1]:.6f}")
    print(f"   - Mejora:  {analisis['mejora_train']:.2f}%")
    
    print(f"\nüìâ Error Validaci√≥n:")
    print(f"   - Inicial: {historial['val_loss'][0]:.6f}")
    print(f"   - Final:   {historial['val_loss'][-1]:.6f}")
    print(f"   - Mejora:  {analisis['mejora_val']:.2f}%")
    
    print(f"\nüéØ An√°lisis:")
    print(f"   - Gap (val - train): {analisis['gap']:.6f}")
    print(f"   - Estado: {analisis['estado']}")
    print(f"   - {analisis['descripcion']}")
    
    print("="*70 + "\n")


def visualizar_ejemplos_dataset(generador, X, y, letras_map, num_ejemplos=6):
    """
    Visualiza ejemplos del dataset comparando originales vs distorsionados.
    
    Par√°metros:
    -----------
    generador : GeneradorDataset
        Instancia del generador para obtener patrones originales
    X : np.ndarray
        Patrones distorsionados
    y : np.ndarray
        Etiquetas one-hot
    letras_map : dict
        Mapeo de √≠ndices a letras {0: 'B', 1: 'D', 2: 'F'}
    num_ejemplos : int
        N√∫mero de ejemplos a mostrar
    """
    fig, axes = plt.subplots(2, num_ejemplos, figsize=(15, 5))
    
    for i in range(num_ejemplos):
        # Obtener la letra real del ejemplo distorsionado
        letra_dist = letras_map[np.argmax(y[i])]
        
        # Generar la versi√≥n PERFECTA de esa misma letra
        X_orig_letra = generador.generar_letra(letra_dist)
        
        # Fila superior: letra perfecta
        axes[0, i].imshow(X_orig_letra.reshape(10, 10), cmap='binary')
        axes[0, i].set_title(f'Original: {letra_dist}')
        axes[0, i].axis('off')
        
        # Fila inferior: letra distorsionada
        axes[1, i].imshow(X[i].reshape(10, 10), cmap='binary')
        # Calcular distorsi√≥n comparando con la versi√≥n perfecta
        dist = np.sum(X[i] != X_orig_letra) / 100 * 100
        axes[1, i].set_title(f'Dist: {dist:.1f}%')
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.show()


def graficar_comparacion_desempeno(acc_train, acc_val, X_train, X_val, y_pred_train, y_pred_val, y_train, y_val):
    """
    Grafica comparaci√≥n de desempe√±o entre entrenamiento y validaci√≥n.
    
    Par√°metros:
    -----------
    acc_train, acc_val : float
        Exactitudes de entrenamiento y validaci√≥n
    X_train, X_val : np.ndarray
        Datos de entrenamiento y validaci√≥n
    y_pred_train, y_pred_val : np.ndarray
        Predicciones del modelo
    y_train, y_val : np.ndarray
        Etiquetas verdaderas
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Gr√°fica 1: Barras de exactitud
    datasets = ['Entrenamiento', 'Validaci√≥n']
    accuracies = [acc_train * 100, acc_val * 100]
    colors = ['#3498db', '#e74c3c']
    
    bars = axes[0].bar(datasets, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)
    axes[0].set_ylabel('Exactitud (%)', fontsize=12, fontweight='bold')
    axes[0].set_title('Exactitud por Conjunto', fontsize=14, fontweight='bold')
    axes[0].set_ylim([0, 105])
    axes[0].grid(axis='y', alpha=0.3)
    
    # Agregar valores sobre las barras
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.2f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    # Agregar l√≠nea de referencia en 100%
    axes[0].axhline(y=100, color='green', linestyle='--', linewidth=2, alpha=0.5, label='100%')
    axes[0].legend()
    
    # Gr√°fica 2: Predicciones correctas vs incorrectas
    correctas_train = int(acc_train * len(X_train))
    incorrectas_train = len(X_train) - correctas_train
    correctas_val = int(acc_val * len(X_val))
    incorrectas_val = len(X_val) - correctas_val
    
    x = np.arange(2)
    width = 0.35
    
    bars1 = axes[1].bar(x - width/2, [correctas_train, correctas_val], width, 
                        label='Correctas', color='#2ecc71', alpha=0.8)
    bars2 = axes[1].bar(x + width/2, [incorrectas_train, incorrectas_val], width,
                        label='Incorrectas', color='#e74c3c', alpha=0.8)
    
    axes[1].set_ylabel('Cantidad de Predicciones', fontsize=12, fontweight='bold')
    axes[1].set_title('Predicciones Correctas vs Incorrectas', fontsize=14, fontweight='bold')
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(['Entrenamiento', 'Validaci√≥n'])
    axes[1].legend()
    axes[1].grid(axis='y', alpha=0.3)
    
    # Agregar valores sobre las barras
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{int(height)}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.show()
    
    # Resumen
    print(f"\nüìà Resumen de Desempe√±o:")
    print(f"   Entrenamiento: {correctas_train}/{len(X_train)} correctas")
    print(f"   Validaci√≥n: {correctas_val}/{len(X_val)} correctas")
    diferencia = abs(acc_train - acc_val)*100
    estado = '‚úÖ Buena generalizaci√≥n' if diferencia < 15 else '‚ö†Ô∏è Posible overfitting'
    print(f"   Diferencia: {diferencia:.2f}% {estado}")


def visualizar_clasificacion_letra(clasificador, letra, distorsion=20):
    """
    Visualiza un ejemplo de clasificaci√≥n con distorsi√≥n.
    
    Par√°metros:
    -----------
    clasificador : ClasificadorLetras
        Instancia del clasificador entrenado
    letra : str
        Letra a clasificar ('B', 'D', o 'F')
    distorsion : int
        Porcentaje de distorsi√≥n (0-30)
    """
    # Generar patrones
    patron_original = clasificador.generador.generar_letra(letra).reshape(10, 10)
    patron_distorsionado = clasificador.generar_patron_distorsionado(letra, distorsion)
    patron_dist_2d = patron_distorsionado.reshape(10, 10)
    
    # Clasificar
    resultado = clasificador.clasificar_patron(patron_distorsionado)
    
    # Visualizar
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Patr√≥n original
    axes[0].imshow(patron_original, cmap='binary', interpolation='nearest')
    axes[0].set_title(f'Patr√≥n Original: {letra}', fontsize=14, fontweight='bold')
    axes[0].axis('off')
    axes[0].grid(True, alpha=0.3)
    
    # Patr√≥n distorsionado
    axes[1].imshow(patron_dist_2d, cmap='binary', interpolation='nearest')
    axes[1].set_title(f'Distorsi√≥n: {distorsion}%', fontsize=14, fontweight='bold')
    axes[1].axis('off')
    axes[1].grid(True, alpha=0.3)
    
    # Resultado de clasificaci√≥n
    axes[2].axis('off')
    resultado_texto = f"""
    RESULTADO DE CLASIFICACI√ìN
    
    Letra Real:       {letra}
    Predicci√≥n:       {resultado['letra']}
    Estado:           {'‚úÖ CORRECTO' if resultado['letra'] == letra else '‚ùå INCORRECTO'}
    
    Confianza:        {resultado['confianza']*100:.1f}%
    
    PROBABILIDADES:
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    B: {resultado['probabilidades'][0]*100:.1f}%
    D: {resultado['probabilidades'][1]*100:.1f}%
    F: {resultado['probabilidades'][2]*100:.1f}%
    """
    
    axes[2].text(0.1, 0.5, resultado_texto, fontsize=12, 
                verticalalignment='center', family='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.show()


def visualizar_ejemplos_originales_vs_distorsionados(generador, X_dist, y_dist, letras_map, num_ejemplos=6):
    """
    Visualiza ejemplos de letras originales vs distorsionadas.
    
    Par√°metros:
    -----------
    generador : GeneradorDataset
        Instancia del generador de datasets
    X_dist : numpy.ndarray
        Array de patrones distorsionados
    y_dist : numpy.ndarray
        Array de etiquetas one-hot
    letras_map : dict
        Diccionario {√≠ndice: letra}
    num_ejemplos : int
        N√∫mero de ejemplos a visualizar
    """
    fig, axes = plt.subplots(2, num_ejemplos, figsize=(15, 5))
    
    for i in range(num_ejemplos):
        # Obtener la letra real del ejemplo distorsionado
        letra_dist = letras_map[np.argmax(y_dist[i])]
        
        # Generar la versi√≥n PERFECTA de esa misma letra
        X_orig_letra = generador.generar_letra(letra_dist)
        
        # Fila superior: letra perfecta
        axes[0, i].imshow(X_orig_letra.reshape(10, 10), cmap='binary')
        axes[0, i].set_title(f'Original: {letra_dist}')
        axes[0, i].axis('off')
        
        # Fila inferior: letra distorsionada
        axes[1, i].imshow(X_dist[i].reshape(10, 10), cmap='binary')
        # Comparar la letra distorsionada con SU versi√≥n perfecta
        dist = np.sum(X_dist[i] != X_orig_letra) / 100 * 100
        axes[1, i].set_title(f'Dist: {dist:.1f}%')
        axes[1, i].axis('off')
    
    plt.tight_layout()
    plt.show()


def evaluar_y_comparar(clasificador, X_train, y_train, X_val, y_val, letras_map):
    """
    Eval√∫a el clasificador en entrenamiento y validaci√≥n, mostrando comparaci√≥n.
    
    Par√°metros:
    -----------
    clasificador : ClasificadorLetras
        Clasificador entrenado
    X_train, y_train : numpy.ndarray
        Datos de entrenamiento
    X_val, y_val : numpy.ndarray
        Datos de validaci√≥n
    letras_map : dict
        Diccionario {√≠ndice: letra}
        
    Returns:
    --------
    dict : Diccionario con m√©tricas {'precision_train', 'precision_val', 'aciertos_train', 'aciertos_val'}
    """
    # Evaluar en ENTRENAMIENTO
    print("üîπ Evaluaci√≥n en conjunto de ENTRENAMIENTO:")
    aciertos_train = 0
    for i in range(len(X_train)):
        resultado = clasificador.clasificar_patron(X_train[i])
        real = letras_map[np.argmax(y_train[i])]
        if resultado['letra'] == real:
            aciertos_train += 1
    
    precision_train = (aciertos_train / len(X_train)) * 100
    print(f"   Precisi√≥n: {precision_train:.2f}% ({aciertos_train}/{len(X_train)})")
    
    # Evaluar en VALIDACI√ìN
    print("\nüîπ Evaluaci√≥n en conjunto de VALIDACI√ìN:")
    aciertos_val = 0
    for i in range(len(X_val)):
        resultado = clasificador.clasificar_patron(X_val[i])
        real = letras_map[np.argmax(y_val[i])]
        if resultado['letra'] == real:
            aciertos_val += 1
    
    precision_val = (aciertos_val / len(X_val)) * 100
    print(f"   Precisi√≥n: {precision_val:.2f}% ({aciertos_val}/{len(X_val)})")
    
    # An√°lisis
    print(f"\nüìà AN√ÅLISIS:")
    print(f"   Diferencia: {abs(precision_train - precision_val):.2f}%")
    if precision_train - precision_val > 10:
        print(f"   ‚ö†Ô∏è  Posible overfitting (modelo memoriza en lugar de aprender)")
    elif precision_val > precision_train:
        print(f"   ‚úÖ Excelente! El modelo generaliza bien")
    else:
        print(f"   ‚úÖ Buen balance entre entrenamiento y validaci√≥n")
    
    return {
        'precision_train': precision_train,
        'precision_val': precision_val,
        'aciertos_train': aciertos_train,
        'aciertos_val': aciertos_val
    }


def visualizar_predicciones_aleatorias(clasificador, X_dist, y_dist, letras_map, num_ejemplos=12):
    """
    Visualiza predicciones aleatorias del clasificador.
    
    Par√°metros:
    -----------
    clasificador : ClasificadorLetras
        Clasificador entrenado
    X_dist : numpy.ndarray
        Array de patrones
    y_dist : numpy.ndarray
        Array de etiquetas one-hot
    letras_map : dict
        Diccionario {√≠ndice: letra}
    num_ejemplos : int
        N√∫mero de ejemplos a mostrar
    """
    indices_random = np.random.choice(len(X_dist), num_ejemplos, replace=False)
    fig, axes = plt.subplots(3, 4, figsize=(16, 12))
    axes = axes.flatten()
    
    for idx, i in enumerate(indices_random):
        resultado = clasificador.clasificar_patron(X_dist[i])
        pred = resultado['letra']
        real = letras_map[np.argmax(y_dist[i])]
        correcto = "[OK]" if pred == real else "[ERROR]"
        color = 'green' if pred == real else 'red'
        
        axes[idx].imshow(X_dist[i].reshape(10, 10), cmap='binary')
        axes[idx].set_title(f'{correcto} Real: {real} | Pred: {pred}', 
                            color=color, fontweight='bold')
        axes[idx].axis('off')
    
    plt.suptitle('Predicciones (Verde=OK, Rojo=Error)', fontsize=14)
    plt.tight_layout()
    plt.show()
    
    return resultado
